{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as scs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "$$ Y = \\alpha + \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon $$\n",
    "\n",
    "where:\n",
    "+ $ \\epsilon \\sim N(0, \\sigma^2) $, $\\sigma$ fixed\n",
    "+ $ \\alpha \\sim N(0, 100) $\n",
    "+ $ \\beta_1 \\sim N(0, 100) $\n",
    "+ $ \\beta_2 \\sim N(0, 100) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters used for simulation\n",
    "alpha = 10\n",
    "beta_1 = -20\n",
    "beta_2 = 15\n",
    "\n",
    "# Constant parameters\n",
    "sigma = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of simulations\n",
    "N = 1000\n",
    "\n",
    "epsilon = np.random.normal(scale=sigma, size = N)\n",
    "X_1 = np.random.rand(N)\n",
    "X_2 = np.random.rand(N)\n",
    "Y = alpha + beta_1 * X_1 + beta_2 * X_2 + epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.scatter(X_1, Y)\n",
    "plt.ylabel('Y')\n",
    "plt.xlabel('X1')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(X_2, Y)\n",
    "plt.ylabel('Y')\n",
    "plt.xlabel('X2')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gibbs sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About Gibbs sampling step\n",
    "+ $ (\\alpha | Y, X_1, X_2, \\beta_1, \\beta_2) \\sim N(\\frac{1}{\\sigma^2}\\frac{\\tau^2\\sigma^2}{\\tau^2 n + \\sigma^2} \\sum_{i=1}^n (Y_i - \\beta_1 X_{1,i} - \\beta_2 X_{2,i}), \\frac{\\tau^2\\sigma^2}{\\tau^2 n + \\sigma^2}) $\n",
    "+ $ (\\beta_1 | Y, X_1, X_2, \\alpha, \\beta_2) \\sim N(\\frac{1}{\\sigma^2}\\frac{\\tau^2\\sigma^2}{\\tau^2 \\sum_{i=1}^n X_{1,i}^2+ \\sigma^2} \\sum_{i=1}^n X_{1,i}(Y_i - \\beta_2 X_{2,i} - \\alpha)) $\n",
    "+ $ (\\beta_2 | Y, X_1, X_2, \\alpha, \\beta_1) \\sim N(\\frac{1}{\\sigma^2}\\frac{\\tau^2\\sigma^2}{\\tau^2 \\sum_{i=1}^n X_{2,i}^2+ \\sigma^2} \\sum_{i=1}^n X_{2,i}(Y_i - \\beta_1 X_{1,i} - \\alpha)) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs_iteration(a, b1, b2, y, x1, x2, sigma = 1):\n",
    "    # Simulation of a_next\n",
    "    n = len(y)\n",
    "    \n",
    "    sigma_a = np.sqrt(100*sigma**2/(100*n+sigma**2))\n",
    "    mu_a = sigma_a**2/sigma**2 * np.sum(y-b1*x1-b2*x2)\n",
    "    a_next = np.random.normal(loc=mu_a, scale=sigma_a)\n",
    "    \n",
    "    sigma_b1 = np.sqrt(sigma**2 * 100 / (100 * np.sum(x1**2) + sigma**2))\n",
    "    mu_b1 = sigma_b1**2/sigma**2 * np.sum(x1*(y-b2*x2 - a_next))\n",
    "    b1_next = np.random.normal(loc=mu_b1, scale=sigma_b1)\n",
    "    \n",
    "    sigma_b2 = np.sqrt(sigma**2 * 100 / (100 * np.sum(x2**2) + sigma**2))\n",
    "    mu_b2 = sigma_b2**2/sigma**2 * np.sum(x2*(y-b1_next*x1 - a_next))\n",
    "    b2_next = np.random.normal(loc=mu_b2, scale=sigma_b2)\n",
    "    \n",
    "    return a_next, b1_next, b2_next\n",
    "\n",
    "def gibbs_sampling(n_iterations, n_drops, a, b1, b2, y, x1, x2, sigma = 1):\n",
    "    S = np.zeros((n_iterations + n_drops, 3))\n",
    "    for k in range(n_iterations + n_drops):\n",
    "        a, b1, b2 = gibbs_iteration(a, b1, b2, y, x1, x2, sigma)\n",
    "        S[k, :] = np.array([a, b1, b2])\n",
    "    return S[n_drops:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "a, b1, b2 = 0, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "n_simul = 20000\n",
    "t = time.time()\n",
    "S1 = gibbs_sampling(n_simul, 5000, a, b1, b2, Y, X_1, X_2, sigma)\n",
    "S2 = gibbs_sampling(n_simul, 5000, a, b1, b2, Y, X_1, X_2, sigma)\n",
    "print(\"Execution time : {} sec for {} simulations\".format((time.time()-t)/2, n_simul))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chaîne 1\n",
    "print(\"Estimation for alpha : {}\".format(np.mean(S1[:,0])))\n",
    "print(\"Estimation for beta_1 : {}\".format(np.mean(S1[:,1])))\n",
    "print(\"Estimation for beta_2 : {}\".format(np.mean(S1[:,2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chaîne 2\n",
    "print(\"Estimation for alpha : {}\".format(np.mean(S2[:,0])))\n",
    "print(\"Estimation for beta_1 : {}\".format(np.mean(S2[:,1])))\n",
    "print(\"Estimation for beta_2 : {}\".format(np.mean(S2[:,2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.hist(S1[:,0], bins=50, normed=True, label = 'Gibbs Sampler 1')\n",
    "plt.hist(S2[:,0], bins=50, normed=True, label = 'Gibbs Sampler 2')\n",
    "plt.axvline(x = alpha, label = \"Theory\", color = 'r')\n",
    "plt.title(\"Posterior distribution for alpha\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.hist(S1[:,1], bins=50, normed=True, label = 'Gibbs Sampler 1')\n",
    "plt.hist(S2[:,1], bins=50, normed=True, label = 'Gibbs Sampler 2')\n",
    "plt.axvline(x = beta_1, label = \"Theory\", color = 'r')\n",
    "plt.title(\"Posterior distribution for beta_1\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.hist(S1[:,2], bins=50, normed=True, label = 'Gibbs Sampler 1')\n",
    "plt.hist(S2[:,2], bins=50, normed=True, label = 'Gibbs Sampler 2')\n",
    "plt.axvline(x = beta_2, label = \"Theory\", color = 'r')\n",
    "plt.title(\"Posterior distribution for beta_2\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metropolis-Hastings algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Density $\\pi$ : $\\pi(\\alpha, \\beta_1, \\beta_2) \\propto \\exp(-\\frac{1}{2 \\sigma^2} \\sum_{i=1}^n (Y_i - \\beta_1 X_{1,i} - \\beta_2 X_{2,i} - \\alpha)^2 - \\frac{\\alpha^2 + \\beta_1^2 + \\beta_2^2}{2})$\n",
    "\n",
    "Transition (gaussian) : $(\\alpha, \\beta_1, \\beta_2) \\rightarrow (\\alpha^{next}, \\beta_1^{next}, \\beta_2^{next}) \\sim (N(\\alpha, \\sigma_t^2), N(\\beta_1, \\sigma_t^2), N(\\beta_2, \\sigma_t^2))  $\n",
    "\n",
    "How to choose $\\sigma_t$? $\\rightarrow$ acceptation rate around 45% (1d), 23% (infty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_pi(a, b1, b2, y, x1, x2, sigma = 1):\n",
    "    return -1/(2*sigma**2)*np.sum((y-b1*x1-b2*x2-a)**2) - a**2/200 - b1**2/200 - b2**2/200\n",
    "\n",
    "def Gaussian_kernel(a, b1, b2, y, x1, x2, sigma = 1, sigma_tr = 0.1):\n",
    "    a_next = np.random.normal(loc = a, scale = sigma_tr)\n",
    "    b1_next = np.random.normal(loc = b1, scale = sigma_tr)\n",
    "    b2_next = np.random.normal(loc = b2, scale = sigma_tr)\n",
    "    acceptation_rate = np.exp(log_pi(a_next, b1_next, b2_next, y, x1, x2, sigma) - log_pi(a, b1, b2, y, x1, x2, sigma))\n",
    "    u = np.random.uniform()\n",
    "    if u<acceptation_rate :\n",
    "        return a_next, b1_next, b2_next, True\n",
    "    else:\n",
    "        return a, b1, b2, False\n",
    "\n",
    "def other_transition(a, b1, b2, y, x1, x2, sigma = 1, lbda = 0.9):\n",
    "    a_next = lbda * a + np.sqrt(1-lbda**2)*np.random.normal()\n",
    "    b1_next = lbda * b1 + np.sqrt(1-lbda**2)*np.random.normal()\n",
    "    b2_next = lbda * b2 + np.sqrt(1-lbda**2)*np.random.normal()\n",
    "    acceptation_rate = np.exp(log_pi(a_next, b1_next, b2_next, y, x1, x2, sigma) \n",
    "                              + np.log(scs.norm.pdf(a - lbda * a_next))\n",
    "                              - log_pi(a, b1, b2, y, x1, x2, sigma)\n",
    "                              - np.log(scs.norm.pdf(a_next - lbda * a)))\n",
    "    u = np.random.uniform()\n",
    "    if u<acceptation_rate :\n",
    "        return a_next, b1_next, b2_next, True\n",
    "    else:\n",
    "        return a, b1, b2, False\n",
    "\n",
    "def MH_sampling(n_iterations, n_drops, a, b1, b2, y, x1, x2, tr_function, sigma = 1, param_tr = 0.1):\n",
    "    S = np.zeros((n_iterations + n_drops, 3))\n",
    "    acceptation_rate = 0\n",
    "    for k in range(n_iterations + n_drops):\n",
    "        a, b1, b2, accepted = tr_function(a, b1, b2, y, x1, x2, sigma, param_tr)\n",
    "        acceptation_rate += int(accepted)\n",
    "        S[k, :] = np.array([a, b1, b2])\n",
    "    mean_acc = round(acceptation_rate/len(S),2)\n",
    "    print(\"Mean acceptation rate : {}\".format(mean_acc))\n",
    "    return (S[n_drops:,:], mean_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "S, accept = MH_sampling(20000, 5000, a, b1, b2, Y, X_1, X_2, Gaussian_kernel, sigma = 1, param_tr = 0.07)\n",
    "print(\"Execution time : {}\".format(time.time()-t))\n",
    "#print(log_pi(a, b1, b2, Y, X_1, X_2, sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.plot(S[:,0])\n",
    "plt.title(\"Markov chain for alpha\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(S[:,1])\n",
    "plt.title(\"Markov chain for beta 1\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.plot(S[:,2])\n",
    "plt.title(\"Markov chain for beta 2\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.hist(S[:,0], bins=50, normed=True, label = 'MH sampler')\n",
    "plt.axvline(x = alpha, label = \"Theory\", color = 'r')\n",
    "plt.title(\"Posterior distribution for alpha\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.hist(S[:,1], bins=50, normed=True, label = 'MH sampler')\n",
    "plt.axvline(x = beta_1, label = \"Theory\", color = 'r')\n",
    "plt.title(\"Posterior distribution for beta_1\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.hist(S[:,2], bins=50, normed=True, label = 'MH sampler')\n",
    "plt.axvline(x = beta_2, label = \"Theory\", color = 'r')\n",
    "plt.title(\"Posterior distribution for beta_2\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison results\n",
    "import pandas as pd\n",
    "stats = pd.DataFrame(columns=[\"Sampling\", \"Mean alpha\", \"Std alpha\", \"Mean beta 1\", \"Std beta 1\", \"Mean beta 2\", \"Std beta 2\"])\n",
    "stats[\"Sampling\"] = [\"Gibbs 1\", \"Gibbs 2\", \"M-H\"]\n",
    "stats[\"Mean alpha\"] = [np.mean(S1[:,0]), np.mean(S2[:,0]), np.mean(S[:,0])]\n",
    "stats[\"Std alpha\"] = [np.std(S1[:,0]), np.std(S2[:,0]), np.std(S[:,0])]\n",
    "stats[\"Mean beta 1\"] = [np.mean(S1[:,1]), np.mean(S2[:,1]), np.mean(S[:,1])]\n",
    "stats[\"Std beta 1\"] = [np.std(S1[:,1]), np.std(S2[:,1]), np.std(S[:,1])]\n",
    "stats[\"Mean beta 2\"] = [np.mean(S1[:,2]), np.mean(S2[:,2]), np.mean(S[:,2])]\n",
    "stats[\"Std beta 2\"] = [np.std(S1[:,2]), np.std(S2[:,2]), np.std(S[:,2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
